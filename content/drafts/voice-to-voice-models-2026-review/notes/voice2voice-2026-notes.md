
# Voice-to-Voice Models 2026 — Research Notes

Last updated: 2026-02-27

---

## Conceptual Foundation

### Audio Tokenization: The Two Camps

Current speech LMs use two primary audio tokenization methods:

| **Aspect** | **Acoustic Tokens** | **Semantic Tokens** |
| ---| ---| --- |
| **Focus** | Low-level acoustic properties | High-level semantic information |
| **Objective** | Efficient compression and reconstruction | Capturing meaningful audio units |
| **Training Approach** | Often unsupervised or self-supervised | Typically self-supervised on large datasets |
| **Interpretability** | Generally less interpretable | Can be more interpretable (e.g., phoneme-like units) |
| **Downstream Tasks** | Better for generation and reconstruction | Better for classification and understanding tasks |
| **Cross-modal Alignment** | Less suitable | More suitable for alignment with text or other modalities |
| **Compression Efficiency** | Usually higher | Can be lower, as semantic information is prioritized |
| **Robustness to Noise** | Often more sensitive to acoustic variations | Can be more robust to minor acoustic changes |
| **Language Dependency** | Generally language-independent | May capture language-specific features |

Good intro: https://ravinkumar.com/GenAiGuidebook/audio/audio_tokenization.html#audio-tokenization-acoustic-tokens-neural-compression

### The Pipeline: ASR → LLM → TTS (Not Dead)

Traditional ASR → LLM → TTS cascade with streaming can achieve sub-1s end-to-end latency in production. It is proven and deployed widely.

**Strengths of the cascade**:
- LLM component operates in text domain — natively supports massive context windows (128K+), tool calls, RAG, structured outputs
- Persona/voice controlled via TTS selection
- Well-understood, easy to debug, many production deployments
- Knowledge injection is trivial — the LLM just sees text

**Actual weaknesses** (not latency per se):
- No true full-duplex — system must detect end-of-turn before responding (sequential)
- No overlapping speech, no natural backchanneling ("uh-huh"), no mid-sentence interruptions
- Non-linguistic cues (tone, hesitation, emotion) lost between ASR and LLM stages
- Turn-taking feels robotic even at sub-1s latency — the conversational *rhythm* is off

---

## Model Landscape

### 1. Moshi — Kyutai (Sept 2024) ⭐ Key reference model

- **Paper**: https://arxiv.org/pdf/2410.00037
- **GitHub**: https://github.com/kyutai-labs/moshi
- **Architecture**: Helium (7B LLM) + Mimi neural audio codec + multi-stream transformer
- **Latency**: 160ms theoretical, 200ms in practice
- **Key innovation**: Full-duplex — models user and system speech simultaneously on parallel streams
- **Inner Monologue**: Predicts time-aligned text tokens as prefix to audio tokens; enables streaming ASR+TTS while improving linguistic quality
- **Mimi codec**: Operates at 12.5Hz, 1.1kbps, designed for LLM token-rate compatibility
- **Training**: Open-source weights, PyTorch / Rust / MLX inference
- **Limitation**: Fixed voice and fixed role — cannot be customized per persona

### 2. NVIDIA PersonaPlex — NVIDIA (Jan 2026) ⭐ New, builds on Moshi

- **Paper**: https://research.nvidia.com/labs/adlr/files/personaplex/personaplex_preprint.pdf
- **HuggingFace**: https://huggingface.co/nvidia/personaplex-7b-v1
- **GitHub**: https://github.com/NVIDIA/personaplex
- **Page**: https://research.nvidia.com/labs/adlr/personaplex/
- **License**: MIT (code) + NVIDIA Open Model License (weights) — commercial use OK
- **Architecture**: Built directly on Moshi architecture (7B params). Adds hybrid prompting system:
  - **Text prompt**: Natural language role description → controls behavior, knowledge, personality
  - **Voice prompt**: Audio embedding → controls vocal characteristics, prosody, accent
  - Mimi speech encoder/decoder + Helium LLM backbone
- **Key innovation**: Disentangles conversational naturalness (from real Fisher conversations) from task-adherence (from synthetic assistant/customer service data)
- **Training data**:
  - 7,303 real conversations (1,217 hours) from Fisher English corpus — back-annotated with GPT-OSS-120B
  - 39,322 synthetic assistant conversations (410 hours)
  - 105,410 synthetic customer service conversations (1,840 hours)
  - Synthetic dialogue generated by Qwen3-32B + GPT-OSS-120B; speech by Chatterbox TTS
- **Benchmarks vs competition** (FullDuplexBench):
  - 100% user interruption success rate (vs Gemini Live 43.9%, Moshi 60.6%)
  - Average latency 0.205s (vs Gemini Live 1.181s, Moshi 0.240s)
  - Task adherence score 4.34/5 (vs Gemini Live 3.68, Moshi 1.26, Qwen2.5-Omni 3.82)
  - Outperforms Gemini Live on dialog naturalness (3.90 vs 3.72)
- **Emergent generalization**: Handles scenarios outside training (astronaut space emergency) — inherited from Helium pretraining
- **Key finding**: Only ~5,000 hours of directed data needed to specialize from Moshi pretrained weights

### 3. Qwen2.5-Omni — Alibaba Qwen Team (March 2025)

- **Paper**: https://arxiv.org/abs/2503.20215
- **Blog**: https://qwenlm.github.io/blog/qwen2.5-omni/
- **GitHub**: https://github.com/QwenLM/Qwen2.5-Omni
- **Architecture**: Thinker-Talker
  - **Thinker**: Multimodal LM processing text, audio, image, video inputs
  - **Talker**: Dual-track autoregressive model generating audio tokens from Thinker's hidden states (streaming)
  - **TMRoPE**: Novel position embedding synchronizing video and audio timestamps
- **Models**: 7B (main), 3B (mobile/edge deployment)
- **Strengths**: Full multimodal (not just voice) — handles image, video, audio, text in one model
- **Performance on FullDuplexBench**: Latency 0.257s avg, task adherence 3.82/5
- **Different design bet**: Thinker thinks in text/hidden states, Talker speaks — not end-to-end audio like Moshi

### 4. MiniCPM-o 2.6 — OpenBMB (2025)

- **Blog**: https://openbmb.notion.site/MiniCPM-o-2-6...
- **Architecture**: End-to-end multimodal, 8B total params
  - Audio encoder: Whisper-medium-300M
  - LLM backbone: Qwen2.5-7B
  - Audio decoder: Auto-regressive lightweight model initialized from ChatTTS-200M (jointly models speech embeddings, text tokens, audio tokens)
  - Vision: SigLip-400M
- **Key feature**: Streaming multimodal live — designed for mobile deployment with full audio+vision+text pipeline

### 5. Mini-Omni / Mini-Omni2 (Aug–Oct 2024)

- **Paper**: https://arxiv.org/abs/2408.16725 / https://arxiv.org/abs/2410.11190
- **GitHub**: https://github.com/gpt-omni
- Earlier proof-of-concept; first fully end-to-end open-source real-time speech LLM
- Mini-Omni2 adds vision + duplex capabilities (closest open-source reproduction of GPT-4o at time of release)
- Now largely superseded by Moshi + PersonaPlex for speech, Qwen2.5-Omni for multimodal

### 6. Ultravox (Fixie AI)

- **GitHub**: https://github.com/fixie-ai/ultravox
- Speech-to-text-to-speech pipeline, open source — not full-duplex

---

## The Architecture Race: Two Bets

| | **Moshi / PersonaPlex** | **Qwen2.5-Omni / MiniCPM-o** |
|---|---|---|
| **Paradigm** | Audio-native end-to-end | LLM core + audio adapters |
| **Duplex** | True full-duplex (simultaneous) | Near-duplex (streaming) |
| **Latency** | 160–240ms | 250–270ms |
| **Persona control** | Yes (PersonaPlex) | Via system prompt |
| **Multimodal** | Audio only (text via inner monologue) | Audio + vision + video + text |
| **Best for** | Conversation naturalism | General multimodal tasks |

---

## Production Use Cases: Where Each Architecture Fits

### Use cases requiring full-duplex naturalism (Moshi / PersonaPlex favored)
- Customer service with natural conversational flow (PersonaPlex's sweet spot)
- Casual voice companions / social AI
- Language conversation practice (not structured tutoring — free-form conversation)
- Coaching where active listening signals matter

### Use cases requiring knowledge grounding (ASR→LLM→TTS or Thinker-Talker favored)
- Conversational tutoring with lesson plans, vocabulary tracking, session history (author's experience building these at Pearson)
- Medical intake requiring structured questionnaires and patient history
- Technical support with troubleshooting trees and documentation lookup
- Personal assistants that need to remember user preferences and past interactions

### Gap analysis
- No current speech-to-speech model handles multi-turn structured goal pursuit well (following a full lesson plan, troubleshooting tree, or intake procedure across turns)
- Pronunciation scoring, vocabulary tracking, CEFR-level progression are domain-specific layers — none of these base models provide them
- The hybrid approach (speech-to-speech for naturalism + external orchestration for goals) is likely the production architecture for structured use cases

---

## Key Benchmarks

- **FullDuplexBench**: https://arxiv.org/abs/2503.04721 — Evaluates turn-taking, user interruption, pause handling, response quality (GPT-4o as judge)
- **ServiceDuplexBench** (NVIDIA): Extension of FullDuplexBench for customer service scenarios (to be released)

---

## Background Resources

- Audio tokenization intro: https://ravinkumar.com/GenAiGuidebook/audio/audio_tokenization.html
- Tincans AI research (real-time voice system, Jan–Apr 2024): https://tincans.ai/report
- Survey paper: "Recent Advances in Speech Language Models" https://arxiv.org/pdf/2410.03751v3 (Aug 2025 edition)
- Gemini Deep Research report v1: https://gemini.google.com/app/6323f0641eff5860

---

## Production Realities

### Knowledge Injection: Context Window vs RAG vs Architecture

**The core need**: A useful voice agent (tutor, customer service rep, personal assistant) must know things about the user — preferences, session history, case details, previous interactions.

**Approach 1: Context window stuffing**
- Put everything into the system prompt / conversation context
- Simplest, zero additional latency
- Problem for end-to-end audio models: audio tokens at 12.5Hz consume context window rapidly. Background knowledge competes with audio history needed for conversational coherence.
- ASR→LLM→TTS cascade wins here: the LLM operates in text domain, handles 128K+ token contexts natively without audio token budget constraints.

**Approach 2: RAG (retrieve dynamically)**
- Standard vector DB lookup: 50–300ms depending on index size and infra
- Full-duplex models target 160–200ms response time → RAG after end-of-turn breaks duplex illusion
- Mitigations: speculative retrieval (stream ASR in background, predict queries before user finishes), pre-load likely contexts at session start, semi-cascaded pipelines (FireRedChat)
- Pure end-to-end audio models (Moshi/PersonaPlex) are architecturally harder to integrate with RAG — audio tokens processed directly, no clean injection point
- Thinker-Talker (Qwen2.5-Omni) is structurally more amenable — inject into Thinker's text/hidden state input, Talker generates speech from enriched representation

**Approach 3: Hybrid orchestration**
- External orchestrator manages session state, user profile, lesson plan (text domain)
- Speech-to-speech model handles the conversation naturalness
- Orchestrator injects context at turn boundaries or during pauses
- Most practical for production structured use cases today

### Open Source vs Closed Source: Trade-offs (No Clear Winner)

**Open source advantages** (PersonaPlex, Qwen2.5-Omni):
- Full control over VAD thresholds, backchanneling, audio codec parameters
- No per-token API cost for audio streaming (significant cost saving at scale)
- Ability to fine-tune on domain-specific data (PersonaPlex showed 5,000h is enough)
- Architecture transparency for debugging production issues

**Closed source advantages** (OpenAI gpt-realtime):
- Regional deployments (especially via Azure) — network latency argument weakens significantly
- Superior script-following and tool-calling capabilities (as of early 2026)
- No GPU infrastructure to manage
- Semantic VAD (classifiers to determine if users are truly finished speaking vs. pausing)

**Important nuance**: Even open source on "edge" still has network latency between user device and edge server (20–50ms). OpenAI via Azure in the same region might achieve 40–80ms network latency with better instruction-following. The decision depends on: scale (API vs GPU costs), team (ML ops capability), domain (fine-tuning needs), and product requirements (natural conversation vs strict script).
